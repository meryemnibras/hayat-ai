import { ChatOpenAI } from '@langchain/openai'
import { HumanMessage, SystemMessage } from '@langchain/core/messages'
import { ENHANCED_SYSTEM_PROMPT } from './langchain-enhanced'

// ØªØ­Ù…ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø©
const OPENAI_API_KEY = process.env.OPENAI_API_KEY
const OPENAI_MODEL = process.env.OPENAI_MODEL || 'gpt-4-turbo-preview'
const TEMPERATURE = parseFloat(process.env.TEMPERATURE || '0.7')
const MAX_TOKENS = parseInt(process.env.MAX_TOKENS || '2000')

if (!OPENAI_API_KEY) {
  throw new Error('âŒ OPENAI_API_KEY ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ .env')
}

/**
 * ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ GPT-4
 */
export const chatModel = new ChatOpenAI({
  openAIApiKey: OPENAI_API_KEY,
  modelName: OPENAI_MODEL,
  temperature: TEMPERATURE,
  maxTokens: MAX_TOKENS,
  streaming: true, // Ù„Ø¯Ø¹Ù… Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ù…ØªØ¯ÙÙ‚Ø©
})

/**
 * Ø¯Ø§Ù„Ø© Ù„Ø¥Ø±Ø³Ø§Ù„ Ø±Ø³Ø§Ù„Ø© ÙˆØ§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø¯
 */
export async function sendMessage(userMessage: string): Promise<string> {
  try {
    const messages = [
      new SystemMessage(ENHANCED_SYSTEM_PROMPT),
      new HumanMessage(userMessage)
    ]
    
    const response = await chatModel.invoke(messages)
    return response.content as string
  } catch (error) {
    console.error('âŒ Ø®Ø·Ø£ ÙÙŠ AI:', error)
    throw error
  }
}

/**
 * Ø¯Ø§Ù„Ø© Ù„Ø¥Ø±Ø³Ø§Ù„ Ù…Ø­Ø§Ø¯Ø«Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø£Ø¯ÙˆØ§Ø±
 */
export async function sendConversation(
  conversationHistory: Array<{ role: 'user' | 'assistant'; content: string }>
): Promise<string> {
  try {
    const messages = [
      new SystemMessage(ENHANCED_SYSTEM_PROMPT),
      ...conversationHistory.map(msg => 
        msg.role === 'user' 
          ? new HumanMessage(msg.content)
          : new SystemMessage(msg.content) // Ø±Ø¯ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ ÙŠØµØ¨Ø­ system message
      )
    ]
    
    const response = await chatModel.invoke(messages)
    return response.content as string
  } catch (error) {
    console.error('âŒ Ø®Ø·Ø£ ÙÙŠ AI:', error)
    throw error
  }
}

/**
 * Ø¯Ø§Ù„Ø© Ù„Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ù…ØªØ¯ÙÙ‚Ø© (Streaming)
 */
export async function* streamMessage(userMessage: string) {
  const messages = [
    new SystemMessage(ENHANCED_SYSTEM_PROMPT),
    new HumanMessage(userMessage)
  ]
  
  const stream = await chatModel.stream(messages)
  
  for await (const chunk of stream) {
    if (chunk.content) {
      yield chunk.content
    }
  }
}

console.log('âœ… Ù†Ø¸Ø§Ù… AI Ø¬Ø§Ù‡Ø²!')
console.log(`ğŸ“¦ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: ${OPENAI_MODEL}`)
console.log(`ğŸŒ¡ï¸ Temperature: ${TEMPERATURE}`)
console.log(`ğŸ“ Max Tokens: ${MAX_TOKENS}`)













